import os
from pprint import pprint
import torch
from silero_vad import (
    load_silero_vad,
    read_audio,
    get_speech_timestamps,
    save_audio,
    collect_chunks
)
import torchaudio
import torchaudio.transforms as T


class SileroVADProcessor:
    def __init__(self, sampling_rate: int = 16000, use_onnx: bool = False):
        self.sampling_rate = sampling_rate
        self.model = load_silero_vad(onnx=use_onnx)
        torch.set_num_threads(1)
    
    def covert_to_clip_timestamp_str(self, speech_timestamps: list) -> str:
        """
        Chuy·ªÉn ƒë·ªïi danh s√°ch c√°c timestamp th√†nh chu·ªói ƒë·ªãnh d·∫°ng "start1,end1,start2,end2,..."
        """
        flat_list = []
        for seg in speech_timestamps:
            flat_list.extend([round(seg["start"], 2), round(seg["end"], 2)])
        
        timestamps_in_seconds = [round(s / self.sampling_rate, 2) for s in flat_list]

        return ",".join(str(x) for x in timestamps_in_seconds)
    
    def save_output(self, input_path: str, output_path: str, speech_timestamps: str):
        """
        L∆∞u k·∫øt qu·∫£ v√†o file
        """
        # ƒê·ªçc to√†n b·ªô audio
        waveform, sr = torchaudio.load(input_path)
        # Chuy·ªÉn ƒë·ªïi n·∫øu kh√°c
        if sr != self.sampling_rate:
            resampler = T.Resample(orig_freq=sr, new_freq=self.sampling_rate)
            waveform = resampler(waveform)

        # C·∫Øt c√°c ƒëo·∫°n v√† gh√©p l·∫°i
        chunks = []
        for seg in speech_timestamps:
            start, end = seg["start"], seg["end"]
            chunk = waveform[:, (int(start) - 1):(int(end) + 1)]
            chunks.append(chunk)

        # Gh√©p l·∫°i th√†nh 1 ƒëo·∫°n duy nh·∫•t
        merged_waveform = torch.cat(chunks, dim=1)  # gh√©p theo chi·ªÅu th·ªùi gian

        # L∆∞u file ƒë√£ gh√©p
        torchaudio.save(output_path, merged_waveform, sample_rate=self.sampling_rate)
        print(f"‚úÖ Saved merged audio to: {output_path}")

    def process(self, input_path: str, output_path: str, show_timestamps: bool = True) -> list:
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        print(f"üì• Reading audio: {input_path}")
        wav = read_audio(input_path, sampling_rate=self.sampling_rate)

        print("üéØ Detecting speech timestamps...")
        speech_timestamps = get_speech_timestamps(wav, self.model, sampling_rate=self.sampling_rate)

        if not speech_timestamps:
            print("‚ö†Ô∏è No speech detected.")
            return []
        
        self.save_output(input_path, output_path, speech_timestamps)

        return self.covert_to_clip_timestamp_str(speech_timestamps)

# if __name__ == "__main__":
#     input_file = "/data/cuongdd/ai_service_copy/model/denoise/mytv_VOD_tiengviet2_chuanhoa_deepfilter.wav"
#     output_file = "/data/cuongdd/ai_service_copy/model/vad_segments/mytv_VOD_tiengviet2_chuanhoa_deepfilter_silero.wav"

#     vad_processor = SileroVADProcessor(sampling_rate=16000, use_onnx=False)
#     result = vad_processor.process(input_file, output_file)

#     print("\n‚úÖ clip_timestamps d√πng cho Whisper:")
#     print(result)
